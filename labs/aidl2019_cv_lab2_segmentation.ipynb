{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aidl2019_cv_lab2_segmentation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "K_Yv_zjZ38Xt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Object Segmentation\n",
        "## Created by [Daniel Fojo](https://www.linkedin.com/in/daniel-fojo/) and [Janna Escur](https://www.linkedin.com/in/janna-escur-i-gelabert-276b1212b/?originalSubdomain=es) \n",
        "##[Postgraduate Course on Artificial Intelligence with Deep Learning](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/). \n",
        "## UPC School, March 2019.\n",
        "This lab will focus on understanding how Mask R-CNN works. To do so, we will use the implementation and the weights from https://github.com/matterport/Mask_RCNN. This repository is one of the most used and tested Mask R-CNNs from Github. This notebook ais also inspired on the demo of this repository.\n",
        "\n",
        "In this lab will test and see how Mask R-CNN work at inference time, and we will even finetune it to work with a toy dataset."
      ]
    },
    {
      "metadata": {
        "id": "U7BxJBcy4THk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/matterport/Mask_RCNN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vJCu8NXQ3wYY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Mask R-CNN - Inspect Trained Model\n",
        "\n",
        "Code and visualizations to test, debug, and evaluate the Mask R-CNN model."
      ]
    },
    {
      "metadata": {
        "id": "tUtVYt323wYb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import cv2\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import skimage\n",
        "\n",
        "# Root directory of the project\n",
        "ROOT_DIR = os.path.abspath(\"./Mask_RCNN\")\n",
        "\n",
        "# Import Mask RCNN\n",
        "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
        "from mrcnn import utils\n",
        "from mrcnn import visualize\n",
        "from mrcnn.config import Config\n",
        "from mrcnn.visualize import display_images\n",
        "import mrcnn.model as modellib\n",
        "from mrcnn.model import log\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "\n",
        "# Local path to trained weights file\n",
        "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "# Download COCO trained weights from Releases if needed\n",
        "if not os.path.exists(COCO_MODEL_PATH):\n",
        "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
        "\n",
        "# Path to Shapes trained weights\n",
        "SHAPES_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_shapes.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uxMR6OSv3wYl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Configurations\n",
        "\n",
        "We have to run the following code to configure our model to do inference with pretrained COCO weights. "
      ]
    },
    {
      "metadata": {
        "id": "kmU4miF83wYo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sys.path.append(os.path.join(ROOT_DIR, \"samples/coco/\"))\n",
        "import coco\n",
        "config = coco.CocoConfig()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ToiOe8G33wYu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Override the training configurations with a few\n",
        "# changes for inferencing.\n",
        "class InferenceConfig(config.__class__):\n",
        "    # Run detection on one image at a time\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "config = InferenceConfig()\n",
        "config.display()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nSyZc8hH3wY1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Notebook Preferences"
      ]
    },
    {
      "metadata": {
        "id": "sHjAVt3C3wY3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Device to load the neural network on.\n",
        "# Useful if you're training a model on the same \n",
        "# machine, in which case use CPU and leave the\n",
        "# GPU for training.\n",
        "DEVICE = \"/gpu:0\"  # /cpu:0 or /gpu:0\n",
        "TEST_MODE = \"inference\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZmdMv1II3wY7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_ax(rows=1, cols=1, size=16):\n",
        "    \"\"\"Return a Matplotlib Axes array to be used in\n",
        "    all visualizations in the notebook. Provide a\n",
        "    central point to control graph sizes.\n",
        "    \n",
        "    Adjust the size attribute to control how big to render images\n",
        "    \"\"\"\n",
        "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
        "    return ax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "snQ4NUW33wZA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## COCO class names"
      ]
    },
    {
      "metadata": {
        "id": "73TLWk7Q8PvT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model classifies objects and returns class IDs, which are integer value that identify each class. Some datasets assign integer values to their classes and some don't. For example, in the MS-COCO dataset, the 'person' class is 1 and 'teddy bear' is 88. The IDs are often sequential, but not always. The COCO dataset, for example, has classes associated with class IDs 70 and 72, but not 71.\n",
        "\n",
        "For our model, we will use the following sequential IDs (that do not match with COCO's IDs). These IDs are the ones that the authors of the repository used."
      ]
    },
    {
      "metadata": {
        "id": "3QY8QhhA8gU8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# COCO Class names\n",
        "# Index of the class in the list is its ID. For example, to get ID of\n",
        "# the teddy bear class, use: class_names.index('teddy bear')\n",
        "class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
        "               'bus', 'train', 'truck', 'boat', 'traffic light',\n",
        "               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
        "               'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
        "               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
        "               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "               'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
        "               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
        "               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
        "               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "               'teddy bear', 'hair drier', 'toothbrush']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ur-WmBdi3wZL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Model\n",
        "\n",
        "We will now load the pretrained weights from COCO."
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "FHVUVNd13wZP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create model in inference mode\n",
        "with tf.device(DEVICE):\n",
        "    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR,\n",
        "                              config=config)\n",
        "\n",
        "# Set weights file path\n",
        "if config.NAME == \"shapes\":\n",
        "    weights_path = SHAPES_MODEL_PATH\n",
        "elif config.NAME == \"coco\":\n",
        "    weights_path = COCO_MODEL_PATH\n",
        "# Or, uncomment to load the last model you trained\n",
        "# weights_path = model.find_last()\n",
        "\n",
        "# Load weights\n",
        "print(\"Loading weights \", weights_path)\n",
        "model.load_weights(weights_path, by_name=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TESYlr1a3wZV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Run Detection\n",
        "\n",
        "The repository includes some sample images. We will test the different steps of our network with them."
      ]
    },
    {
      "metadata": {
        "id": "BM8biK_53wZX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "IMAGE_DIR = os.path.join(ROOT_DIR, \"images\")\n",
        "\n",
        "# Load a random image from the images folder\n",
        "file_names = next(os.walk(IMAGE_DIR))[2]\n",
        "image = skimage.io.imread(os.path.join(IMAGE_DIR, random.choice(file_names)))\n",
        "\n",
        "# Run detection\n",
        "results = model.detect([image], verbose=1)\n",
        "\n",
        "# Visualize results\n",
        "r = results[0]\n",
        "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
        "                            class_names, r['scores'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1b-IMYWIGEvB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 1\n",
        "Test the network with different images taken from Google Images and see the results. Take into account that the network can only predict classes from COCO.\n"
      ]
    },
    {
      "metadata": {
        "id": "7qRjq0NsGP64",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "urllib.request.urlretrieve(\"https://3c1703fe8d.site.internapcdn.net/newman/gfx/news/hires/2018/2-dog.jpg\", \"image.jpg\")\n",
        "image = skimage.io.imread(\"image.jpg\")\n",
        "\n",
        "# Run detection\n",
        "results = model.detect([image], verbose=1)\n",
        "\n",
        "# Visualize results\n",
        "r = results[0]\n",
        "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
        "                            class_names, r['scores'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s5tOhi533wZy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stage 1: Region Proposal Network\n",
        "\n",
        "The Region Proposal Network (RPN) runs a lightweight binary classifier on a lot of boxes (anchors) over the image and returns object/no-object scores. Anchors with high *objectness* score (positive anchors) are passed to the stage two to be classified.\n",
        "\n",
        "Often, even positive anchors don't cover objects fully. So the RPN also regresses a refinement (a delta in location and size) to be applied to the anchors to shift it and resize it a bit to the correct boundaries of the object."
      ]
    },
    {
      "metadata": {
        "id": "bX88n3PG3waE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.a RPN Predictions\n",
        "\n",
        "Here we run the RPN graph and display its predictions."
      ]
    },
    {
      "metadata": {
        "id": "ddjlXpnC3waG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "IMAGE_DIR = os.path.join(ROOT_DIR, \"images\")\n",
        "\n",
        "# Load a random image from the images folder\n",
        "file_names = next(os.walk(IMAGE_DIR))[2]\n",
        "image = skimage.io.imread(os.path.join(IMAGE_DIR, random.choice(file_names)))\n",
        "\n",
        "# Run RPN sub-graph\n",
        "pillar = model.keras_model.get_layer(\"ROI\").output  # node to start searching from\n",
        "h, w = image.shape[:2]\n",
        "# TF 1.4 and 1.9 introduce new versions of NMS. Search for all names to support TF 1.3~1.10\n",
        "nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression:0\")\n",
        "# if nms_node is None:\n",
        "#     nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression/NonMaxSuppressionV2:0\")\n",
        "if nms_node is None: #TF 1.9-1.10\n",
        "    nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression/NonMaxSuppressionV3:0\")\n",
        "\n",
        "rpn = model.run_graph([image], [\n",
        "    (\"rpn_class\", model.keras_model.get_layer(\"rpn_class\").output),\n",
        "    (\"pre_nms_anchors\", model.ancestor(pillar, \"ROI/pre_nms_anchors:0\")),\n",
        "    (\"refined_anchors\", model.ancestor(pillar, \"ROI/refined_anchors:0\")),\n",
        "    (\"refined_anchors_clipped\", model.ancestor(pillar, \"ROI/refined_anchors_clipped:0\")),\n",
        "    (\"post_nms_anchor_ix\", nms_node),\n",
        "    (\"proposals\", model.keras_model.get_layer(\"ROI\").output),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "iSGy0jSM3waU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Show top anchors with refinement. Then with clipping to image boundaries\n",
        "limit = 50\n",
        "ax = get_ax(1, 2)\n",
        "shape = (image.shape[0], image.shape[1])\n",
        "pre_nms_anchors = utils.denorm_boxes(rpn[\"pre_nms_anchors\"][0], shape)\n",
        "refined_anchors = utils.denorm_boxes(rpn[\"refined_anchors\"][0], shape)\n",
        "refined_anchors_clipped = utils.denorm_boxes(rpn[\"refined_anchors_clipped\"][0], shape)\n",
        "visualize.draw_boxes(image, boxes=pre_nms_anchors[:limit],\n",
        "                     refined_boxes=refined_anchors[:limit], ax=ax[0])\n",
        "visualize.draw_boxes(image, refined_boxes=refined_anchors_clipped[:limit], ax=ax[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "HDPhvfHb3waZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Show refined anchors after non-max suppression\n",
        "limit = 50\n",
        "ixs = rpn[\"post_nms_anchor_ix\"][:limit]\n",
        "visualize.draw_boxes(image, refined_boxes=refined_anchors_clipped[ixs], ax=get_ax())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JyH5BNg_3wad",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Show final proposals\n",
        "# These are the same as the previous step (refined anchors \n",
        "# after NMS) but with coordinates normalized to [0, 1] range.\n",
        "limit = 50\n",
        "# Convert back to image coordinates for display\n",
        "h, w = image.shape[:2]\n",
        "proposals = rpn['proposals'][0, :limit] * np.array([h, w, h, w])\n",
        "visualize.draw_boxes(image, refined_boxes=proposals, ax=get_ax())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BACDVoE23wao",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stage 2: Proposal Classification\n",
        "\n",
        "This stage takes the region proposals from the RPN and classifies them."
      ]
    },
    {
      "metadata": {
        "id": "GibZQWlE3wap",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.a Proposal Classification\n",
        "\n",
        "Run the classifier heads on proposals to generate class propbabilities and bounding box regressions."
      ]
    },
    {
      "metadata": {
        "id": "et9jYER63wap",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get input and output to classifier and mask heads.\n",
        "mrcnn = model.run_graph([image], [\n",
        "    (\"proposals\", model.keras_model.get_layer(\"ROI\").output),\n",
        "    (\"probs\", model.keras_model.get_layer(\"mrcnn_class\").output),\n",
        "    (\"deltas\", model.keras_model.get_layer(\"mrcnn_bbox\").output),\n",
        "    (\"masks\", model.keras_model.get_layer(\"mrcnn_mask\").output),\n",
        "    (\"detections\", model.keras_model.get_layer(\"mrcnn_detection\").output),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xKFsaUDr3wat",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get detection class IDs. Trim zero padding.\n",
        "det_class_ids = mrcnn['detections'][0, :, 4].astype(np.int32)\n",
        "det_count = np.where(det_class_ids == 0)[0][0]\n",
        "det_class_ids = det_class_ids[:det_count]\n",
        "detections = mrcnn['detections'][0, :det_count]\n",
        "\n",
        "print(\"{} detections: {}\".format(\n",
        "    det_count, np.array(class_names)[det_class_ids]))\n",
        "\n",
        "captions = [\"{} {:.3f}\".format(class_names[int(c)], s) if c > 0 else \"\"\n",
        "            for c, s in zip(detections[:, 4], detections[:, 5])]\n",
        "visualize.draw_boxes(\n",
        "    image, \n",
        "    refined_boxes=utils.denorm_boxes(detections[:, :4], image.shape[:2]),\n",
        "    visibilities=[2] * len(detections),\n",
        "    captions=captions, title=\"Detections\",\n",
        "    ax=get_ax())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oqdkEzTx3way",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.b Step by Step Detection\n",
        "\n",
        "Here we dive deeper into the process of processing the detections."
      ]
    },
    {
      "metadata": {
        "id": "tmKDs1U13waz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Proposals are in normalized coordinates. Scale them\n",
        "# to image coordinates.\n",
        "h, w = image.shape[:2]\n",
        "proposals = np.around(mrcnn[\"proposals\"][0] * np.array([h, w, h, w])).astype(np.int32)\n",
        "\n",
        "# Class ID, score, and mask per proposal\n",
        "roi_class_ids = np.argmax(mrcnn[\"probs\"][0], axis=1)\n",
        "roi_scores = mrcnn[\"probs\"][0, np.arange(roi_class_ids.shape[0]), roi_class_ids]\n",
        "roi_class_names = np.array(class_names)[roi_class_ids]\n",
        "roi_positive_ixs = np.where(roi_class_ids > 0)[0]\n",
        "\n",
        "# How many ROIs vs empty rows?\n",
        "print(\"{} Valid proposals out of {}\".format(np.sum(np.any(proposals, axis=1)), proposals.shape[0]))\n",
        "print(\"{} Positive ROIs\".format(len(roi_positive_ixs)))\n",
        "\n",
        "# Class counts\n",
        "print(list(zip(*np.unique(roi_class_names, return_counts=True))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CPdZ4mrY3wa3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Display a random sample of proposals.\n",
        "# Proposals classified as background are dotted, and\n",
        "# the rest show their class and confidence score.\n",
        "h, w = image.shape[:2]\n",
        "limit = 200\n",
        "ixs = np.random.randint(0, proposals.shape[0], limit)\n",
        "captions = [\"{} {:.3f}\".format(class_names[c], s) if c > 0 else \"\"\n",
        "            for c, s in zip(roi_class_ids[ixs], roi_scores[ixs])]\n",
        "visualize.draw_boxes(image, boxes=proposals[ixs],\n",
        "                     visibilities=np.where(roi_class_ids[ixs] > 0, 2, 1),\n",
        "                     captions=captions, title=\"ROIs Before Refinement\",\n",
        "                     ax=get_ax())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TLh_ilDK3wa9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Apply Bounding Box Refinement"
      ]
    },
    {
      "metadata": {
        "id": "fOHscWP73wa9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Class-specific bounding box shifts.\n",
        "roi_bbox_specific = mrcnn[\"deltas\"][0, np.arange(proposals.shape[0]), roi_class_ids]\n",
        "log(\"roi_bbox_specific\", roi_bbox_specific)\n",
        "\n",
        "# Apply bounding box transformations\n",
        "# Shape: [N, (y1, x1, y2, x2)]\n",
        "refined_proposals = utils.apply_box_deltas(\n",
        "    proposals, roi_bbox_specific * config.BBOX_STD_DEV).astype(np.int32)\n",
        "log(\"refined_proposals\", refined_proposals)\n",
        "\n",
        "# Show positive proposals\n",
        "# ids = np.arange(roi_boxes.shape[0])  # Display all\n",
        "limit = 5\n",
        "ids = np.random.randint(0, len(roi_positive_ixs), limit)  # Display random sample\n",
        "captions = [\"{} {:.3f}\".format(class_names[c], s) if c > 0 else \"\"\n",
        "            for c, s in zip(roi_class_ids[roi_positive_ixs][ids], roi_scores[roi_positive_ixs][ids])]\n",
        "visualize.draw_boxes(image, boxes=proposals[roi_positive_ixs][ids],\n",
        "                     refined_boxes=refined_proposals[roi_positive_ixs][ids],\n",
        "                     visibilities=np.where(roi_class_ids[roi_positive_ixs][ids] > 0, 1, 0),\n",
        "                     captions=captions, title=\"ROIs After Refinement\",\n",
        "                     ax=get_ax())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8XROUc8J3wbA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Filter Low Confidence Detections"
      ]
    },
    {
      "metadata": {
        "id": "lS-JBDqU3wbB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Remove boxes classified as background\n",
        "keep = np.where(roi_class_ids > 0)[0]\n",
        "print(\"Keep {} detections:\\n{}\".format(keep.shape[0], keep))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tlKyGq6m3wbF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Remove low confidence detections\n",
        "keep = np.intersect1d(keep, np.where(roi_scores >= config.DETECTION_MIN_CONFIDENCE)[0])\n",
        "print(\"Remove boxes below {} confidence. Keep {}:\\n{}\".format(\n",
        "    config.DETECTION_MIN_CONFIDENCE, keep.shape[0], keep))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4OJb4Zrq3wbL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Per-Class Non-Max Suppression"
      ]
    },
    {
      "metadata": {
        "id": "EZNkNCv13wbM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Apply per-class non-max suppression\n",
        "pre_nms_boxes = refined_proposals[keep]\n",
        "pre_nms_scores = roi_scores[keep]\n",
        "pre_nms_class_ids = roi_class_ids[keep]\n",
        "\n",
        "nms_keep = []\n",
        "for class_id in np.unique(pre_nms_class_ids):\n",
        "    # Pick detections of this class\n",
        "    ixs = np.where(pre_nms_class_ids == class_id)[0]\n",
        "    # Apply NMS\n",
        "    class_keep = utils.non_max_suppression(pre_nms_boxes[ixs], \n",
        "                                            pre_nms_scores[ixs],\n",
        "                                            config.DETECTION_NMS_THRESHOLD)\n",
        "    # Map indicies\n",
        "    class_keep = keep[ixs[class_keep]]\n",
        "    nms_keep = np.union1d(nms_keep, class_keep)\n",
        "    print(\"{:22}: {} -> {}\".format(class_names[class_id][:20], \n",
        "                                   keep[ixs], class_keep))\n",
        "\n",
        "keep = np.intersect1d(keep, nms_keep).astype(np.int32)\n",
        "print(\"\\nKept after per-class NMS: {}\\n{}\".format(keep.shape[0], keep))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aIKFkdu93wbT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Show final detections\n",
        "ixs = np.arange(len(keep))  # Display all\n",
        "# ixs = np.random.randint(0, len(keep), 10)  # Display random sample\n",
        "captions = [\"{} {:.3f}\".format(class_names[c], s) if c > 0 else \"\"\n",
        "            for c, s in zip(roi_class_ids[keep][ixs], roi_scores[keep][ixs])]\n",
        "visualize.draw_boxes(\n",
        "    image, boxes=proposals[keep][ixs],\n",
        "    refined_boxes=refined_proposals[keep][ixs],\n",
        "    visibilities=np.where(roi_class_ids[keep][ixs] > 0, 1, 0),\n",
        "    captions=captions, title=\"Detections after NMS\",\n",
        "    ax=get_ax())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bxoFUwzM3wbV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stage 3: Generating Masks\n",
        "\n",
        "This stage takes the detections (refined bounding boxes and class IDs) from the previous layer and runs the mask head to generate segmentation masks for every instance."
      ]
    },
    {
      "metadata": {
        "id": "qaeStZOp3wba",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.a Predicted Masks"
      ]
    },
    {
      "metadata": {
        "id": "zIrxQIw93wbb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get predictions of mask head\n",
        "mrcnn = model.run_graph([image], [\n",
        "    (\"detections\", model.keras_model.get_layer(\"mrcnn_detection\").output),\n",
        "    (\"masks\", model.keras_model.get_layer(\"mrcnn_mask\").output),\n",
        "])\n",
        "\n",
        "# Get detection class IDs. Trim zero padding.\n",
        "det_class_ids = mrcnn['detections'][0, :, 4].astype(np.int32)\n",
        "det_count = np.where(det_class_ids == 0)[0][0]\n",
        "det_class_ids = det_class_ids[:det_count]\n",
        "\n",
        "print(\"{} detections: {}\".format(\n",
        "    det_count, np.array(class_names)[det_class_ids]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ntHYZodF3wbg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Masks\n",
        "det_boxes = utils.denorm_boxes(mrcnn[\"detections\"][0, :, :4], image.shape[:2])\n",
        "det_mask_specific = np.array([mrcnn[\"masks\"][0, i, :, :, c] \n",
        "                              for i, c in enumerate(det_class_ids)])\n",
        "det_masks = np.array([utils.unmold_mask(m, det_boxes[i], image.shape)\n",
        "                      for i, m in enumerate(det_mask_specific)])\n",
        "log(\"det_mask_specific\", det_mask_specific)\n",
        "log(\"det_masks\", det_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t8ynJLdE3wbj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "display_images(det_mask_specific[:6] * 255, cmap=\"Blues\", interpolation=\"none\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o18gbcPp3wbl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "display_images(det_masks[:4] * 255, cmap=\"Blues\", interpolation=\"none\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ge6hyO563wbn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize Activations\n",
        "\n",
        "In some cases it helps to look at the output from different layers and visualize them to catch issues and odd patterns."
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "8TbbanVz3wbn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get activations of a few sample layers\n",
        "activations = model.run_graph([image], [\n",
        "    (\"input_image\",        tf.identity(model.keras_model.get_layer(\"input_image\").output)),\n",
        "    (\"res4w_out\",          model.keras_model.get_layer(\"res4w_out\").output),  # for resnet100\n",
        "    (\"rpn_bbox\",           model.keras_model.get_layer(\"rpn_bbox\").output),\n",
        "    (\"roi\",                model.keras_model.get_layer(\"ROI\").output),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A368Gge33wbq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Input image (normalized)\n",
        "_ = plt.imshow(modellib.unmold_image(activations[\"input_image\"][0],config))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GssPJEet3wbs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Backbone feature map\n",
        "display_images(np.transpose(activations[\"res4w_out\"][0,:,:,:4], [2, 0, 1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e0UpKzmm3wb4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Histograms of RPN bounding box deltas\n",
        "plt.figure(figsize=(12, 3))\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.title(\"dy\")\n",
        "_ = plt.hist(activations[\"rpn_bbox\"][0,:,0], 50)\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.title(\"dx\")\n",
        "_ = plt.hist(activations[\"rpn_bbox\"][0,:,1], 50)\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.title(\"dw\")\n",
        "_ = plt.hist(activations[\"rpn_bbox\"][0,:,2], 50)\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.title(\"dh\")\n",
        "_ = plt.hist(activations[\"rpn_bbox\"][0,:,3], 50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CJUUu4RX3wb7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Distribution of y, x coordinates of generated proposals\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"y1, x1\")\n",
        "plt.scatter(activations[\"roi\"][0,:,0], activations[\"roi\"][0,:,1])\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"y2, x2\")\n",
        "plt.scatter(activations[\"roi\"][0,:,2], activations[\"roi\"][0,:,3])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pKpNGjJ6A_3b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Mask R-CNN - Train on Shapes Dataset\n",
        "\n",
        "\n",
        "Now we will show how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
        "\n",
        "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
      ]
    },
    {
      "metadata": {
        "id": "9Jgxe97jA_3p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Configurations\n",
        "\n",
        "We need to configure the network for the new dataset."
      ]
    },
    {
      "metadata": {
        "id": "14ykTh0lA_3r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ShapesConfig(Config):\n",
        "    \"\"\"Configuration for training on the toy shapes dataset.\n",
        "    Derives from the base Config class and overrides values specific\n",
        "    to the toy shapes dataset.\n",
        "    \"\"\"\n",
        "    # Give the configuration a recognizable name\n",
        "    NAME = \"shapes\"\n",
        "\n",
        "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
        "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 8\n",
        "\n",
        "    # Number of classes (including background)\n",
        "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
        "\n",
        "    # Use small images for faster training. Set the limits of the small side\n",
        "    # the large side, and that determines the image shape.\n",
        "    IMAGE_MIN_DIM = 128\n",
        "    IMAGE_MAX_DIM = 128\n",
        "\n",
        "    # Use smaller anchors because our image and objects are small\n",
        "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
        "\n",
        "    # Reduce training ROIs per image because the images are small and have\n",
        "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
        "    TRAIN_ROIS_PER_IMAGE = 32\n",
        "\n",
        "    # Use a small epoch since the data is simple\n",
        "    STEPS_PER_EPOCH = 100\n",
        "\n",
        "    # use small validation steps since the epoch is small\n",
        "    VALIDATION_STEPS = 5\n",
        "    \n",
        "config = ShapesConfig()\n",
        "config.display()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QZ8Kr-apA_35",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "Create a synthetic dataset\n",
        "\n",
        "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
        "\n",
        "* load_image()\n",
        "* load_mask()\n",
        "* image_reference()"
      ]
    },
    {
      "metadata": {
        "id": "WxtgZt2UA_36",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ShapesDataset(utils.Dataset):\n",
        "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
        "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
        "    The images are generated on the fly. No file access required.\n",
        "    \"\"\"\n",
        "\n",
        "    def load_shapes(self, count, height, width):\n",
        "        \"\"\"Generate the requested number of synthetic images.\n",
        "        count: number of images to generate.\n",
        "        height, width: the size of the generated images.\n",
        "        \"\"\"\n",
        "        # Add classes\n",
        "        self.add_class(\"shapes\", 1, \"square\")\n",
        "        self.add_class(\"shapes\", 2, \"circle\")\n",
        "        self.add_class(\"shapes\", 3, \"triangle\")\n",
        "\n",
        "        # Add images\n",
        "        # Generate random specifications of images (i.e. color and\n",
        "        # list of shapes sizes and locations). This is more compact than\n",
        "        # actual images. Images are generated on the fly in load_image().\n",
        "        for i in range(count):\n",
        "            bg_color, shapes = self.random_image(height, width)\n",
        "            self.add_image(\"shapes\", image_id=i, path=None,\n",
        "                           width=width, height=height,\n",
        "                           bg_color=bg_color, shapes=shapes)\n",
        "\n",
        "    def load_image(self, image_id):\n",
        "        \"\"\"Generate an image from the specs of the given image ID.\n",
        "        Typically this function loads the image from a file, but\n",
        "        in this case it generates the image on the fly from the\n",
        "        specs in image_info.\n",
        "        \"\"\"\n",
        "        info = self.image_info[image_id]\n",
        "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
        "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
        "        image = image * bg_color.astype(np.uint8)\n",
        "        for shape, color, dims in info['shapes']:\n",
        "            image = self.draw_shape(image, shape, dims, color)\n",
        "        return image\n",
        "\n",
        "    def image_reference(self, image_id):\n",
        "        \"\"\"Return the shapes data of the image.\"\"\"\n",
        "        info = self.image_info[image_id]\n",
        "        if info[\"source\"] == \"shapes\":\n",
        "            return info[\"shapes\"]\n",
        "        else:\n",
        "            super(self.__class__).image_reference(self, image_id)\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
        "        \"\"\"\n",
        "        info = self.image_info[image_id]\n",
        "        shapes = info['shapes']\n",
        "        count = len(shapes)\n",
        "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
        "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
        "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
        "                                                shape, dims, 1)\n",
        "        # Handle occlusions\n",
        "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
        "        for i in range(count-2, -1, -1):\n",
        "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
        "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
        "        # Map class names to class IDs.\n",
        "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
        "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
        "\n",
        "    def draw_shape(self, image, shape, dims, color):\n",
        "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
        "        # Get the center x, y and the size s\n",
        "        x, y, s = dims\n",
        "        if shape == 'square':\n",
        "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
        "        elif shape == \"circle\":\n",
        "            cv2.circle(image, (x, y), s, color, -1)\n",
        "        elif shape == \"triangle\":\n",
        "            points = np.array([[(x, y-s),\n",
        "                                (x-s/math.sin(math.radians(60)), y+s),\n",
        "                                (x+s/math.sin(math.radians(60)), y+s),\n",
        "                                ]], dtype=np.int32)\n",
        "            cv2.fillPoly(image, points, color)\n",
        "        return image\n",
        "\n",
        "    def random_shape(self, height, width):\n",
        "        \"\"\"Generates specifications of a random shape that lies within\n",
        "        the given height and width boundaries.\n",
        "        Returns a tuple of three valus:\n",
        "        * The shape name (square, circle, ...)\n",
        "        * Shape color: a tuple of 3 values, RGB.\n",
        "        * Shape dimensions: A tuple of values that define the shape size\n",
        "                            and location. Differs per shape type.\n",
        "        \"\"\"\n",
        "        # Shape\n",
        "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
        "        # Color\n",
        "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
        "        # Center x, y\n",
        "        buffer = 20\n",
        "        y = random.randint(buffer, height - buffer - 1)\n",
        "        x = random.randint(buffer, width - buffer - 1)\n",
        "        # Size\n",
        "        s = random.randint(buffer, height//4)\n",
        "        return shape, color, (x, y, s)\n",
        "\n",
        "    def random_image(self, height, width):\n",
        "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
        "        Returns the background color of the image and a list of shape\n",
        "        specifications that can be used to draw the image.\n",
        "        \"\"\"\n",
        "        # Pick random background color\n",
        "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
        "        # Generate a few random shapes and record their\n",
        "        # bounding boxes\n",
        "        shapes = []\n",
        "        boxes = []\n",
        "        N = random.randint(1, 4)\n",
        "        for _ in range(N):\n",
        "            shape, color, dims = self.random_shape(height, width)\n",
        "            shapes.append((shape, color, dims))\n",
        "            x, y, s = dims\n",
        "            boxes.append([y-s, x-s, y+s, x+s])\n",
        "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
        "        # shapes covering each other\n",
        "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
        "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
        "        return bg_color, shapes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hVtxKtGaA_3-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training dataset\n",
        "dataset_train = ShapesDataset()\n",
        "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
        "dataset_train.prepare()\n",
        "\n",
        "# Validation dataset\n",
        "dataset_val = ShapesDataset()\n",
        "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
        "dataset_val.prepare()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1wI1kI34Ip8j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will display some images of our toy dataset."
      ]
    },
    {
      "metadata": {
        "id": "OJcB0tLgA_4E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load and display random samples\n",
        "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
        "for image_id in image_ids:\n",
        "    image = dataset_train.load_image(image_id)\n",
        "    mask, class_ids = dataset_train.load_mask(image_id)\n",
        "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sTxnZQfZA_4M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create Model"
      ]
    },
    {
      "metadata": {
        "id": "DkH9sq3TIupn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have to load the model in training mode. We will use weights pretrained with COCO. Feel free to test also imagenet weights by changing the variable `init_with`"
      ]
    },
    {
      "metadata": {
        "id": "LJy8PQO6A_4P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create model in training mode\n",
        "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
        "                          model_dir=MODEL_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "LCiYW3tCA_4W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Which weights to start with?\n",
        "init_with = \"coco\"  # imagenet, coco, or last\n",
        "\n",
        "if init_with == \"imagenet\":\n",
        "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
        "elif init_with == \"coco\":\n",
        "    # Load weights trained on MS COCO, but skip layers that\n",
        "    # are different due to the different number of classes\n",
        "    # See README for instructions to download the COCO weights\n",
        "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
        "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
        "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "elif init_with == \"last\":\n",
        "    # Load the last model you trained and continue training\n",
        "    model.load_weights(model.find_last(), by_name=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GVGOiZxJA_4a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Train in two stages:\n",
        "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function. We will do this for 1 epoch.\n",
        "\n",
        "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers. For this step we will use a lower LR. We will fine-tune for 2 epochs."
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "qeOQ54RuA_4c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train the head branches\n",
        "# Passing layers=\"heads\" freezes all layers except the head\n",
        "# layers. You can also pass a regular expression to select\n",
        "# which layers to train by name pattern.\n",
        "model.train(dataset_train, dataset_val, \n",
        "            learning_rate=config.LEARNING_RATE, \n",
        "            epochs=1, \n",
        "            layers='heads')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "JTfhAQXfA_4h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Fine tune all layers\n",
        "# Passing layers=\"all\" trains all layers. You can also \n",
        "# pass a regular expression to select which layers to\n",
        "# train by name pattern.\n",
        "model.train(dataset_train, dataset_val, \n",
        "            learning_rate=config.LEARNING_RATE / 10,\n",
        "            epochs=2, \n",
        "            layers=\"all\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JUHgFKpIA_4t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Detection"
      ]
    },
    {
      "metadata": {
        "id": "Szp0PR17ELrg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will now load the detection configuration, and test our network in a random image from our generated dataset."
      ]
    },
    {
      "metadata": {
        "id": "fEcFA18nA_4u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class InferenceConfig(ShapesConfig):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "inference_config = InferenceConfig()\n",
        "\n",
        "# Recreate the model in inference mode\n",
        "model = modellib.MaskRCNN(mode=\"inference\", \n",
        "                          config=inference_config,\n",
        "                          model_dir=MODEL_DIR)\n",
        "\n",
        "# Get path to saved weights\n",
        "# Either set a specific path or find last trained weights\n",
        "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
        "model_path = model.find_last()\n",
        "\n",
        "# Load trained weights\n",
        "print(\"Loading weights from \", model_path)\n",
        "model.load_weights(model_path, by_name=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8A5vND5BA_4y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test on a random image\n",
        "image_id = random.choice(dataset_val.image_ids)\n",
        "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "    modellib.load_image_gt(dataset_val, inference_config, \n",
        "                           image_id, use_mini_mask=False)\n",
        "\n",
        "log(\"original_image\", original_image)\n",
        "log(\"image_meta\", image_meta)\n",
        "log(\"gt_class_id\", gt_class_id)\n",
        "log(\"gt_bbox\", gt_bbox)\n",
        "log(\"gt_mask\", gt_mask)\n",
        "\n",
        "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
        "                            dataset_train.class_names, figsize=(8, 8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R44pBRT9A_45",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "results = model.detect([original_image], verbose=1)\n",
        "\n",
        "r = results[0]\n",
        "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
        "                            dataset_val.class_names, r['scores'], ax=get_ax())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0lgUkBctEXNZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can see that with just 3 epochs of training we obtain decent results."
      ]
    },
    {
      "metadata": {
        "id": "hFR_Er_TA_5C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "We will calculate our mean Average Precissio (mAP) with Intersection over Union of 50% (IoU @ 0.5) for 10 images."
      ]
    },
    {
      "metadata": {
        "id": "Cj1PZtsiA_5F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Compute VOC-Style mAP @ IoU=0.5\n",
        "# Running on 10 images. Increase for better accuracy.\n",
        "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
        "APs = []\n",
        "for image_id in image_ids:\n",
        "    # Load image and ground truth data\n",
        "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "        modellib.load_image_gt(dataset_val, inference_config,\n",
        "                               image_id, use_mini_mask=False)\n",
        "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
        "    # Run object detection\n",
        "    results = model.detect([image], verbose=0)\n",
        "    r = results[0]\n",
        "    # Compute AP\n",
        "    AP, precisions, recalls, overlaps =\\\n",
        "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
        "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
        "    APs.append(AP)\n",
        "    \n",
        "print(\"mAP: \", np.mean(APs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NI3P4O-zEGYj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}