{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[AIDL 2019] CV7 Simple GAN - MNIST - with TODO.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"MbRVQ73oRVAO","colab_type":"text"},"source":["# Simple GAN - MNIST  (with TODO)\n","\n","---\n","\n","In this notebook will learn about Generative Adversarial Networks by implementing a simple GAN to generate MNIST digits from noise.\n","\n","**Important:** Set the Cloab environment to run on GPU\n","\n","Author: Albert Pumarola\n","\n","For any doubt or the answers contact me at: apumarola@iri.upc.edu"]},{"cell_type":"code","metadata":{"id":"NcZGU22N9IX2","colab_type":"code","colab":{}},"source":["%load_ext autoreload\n","%matplotlib inline\n","%autoreload 2\n","\n","import torch\n","from torch import nn, optim\n","from torch.autograd.variable import Variable\n","from torchvision import transforms, datasets, utils\n","from IPython.display import display\n","from PIL import Image\n","import numpy as np\n","import math"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1QxHcC_iDR9i","colab_type":"text"},"source":["## Utils\n","Some utils with minor importance"]},{"cell_type":"code","metadata":{"id":"iiTaSkPaDV8r","colab_type":"code","colab":{}},"source":["def norm_noise(size):\n","    return torch.cuda.FloatTensor(size, 100).normal_()\n","  \n","def init_weights(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1 or classname.find('BatchNorm') != -1:\n","        m.weight.data.normal_(0.00, 0.02)\n","  \n","def display_batch_images(img, imtype=np.uint8, unnormalize=True, nrows=None, mean=0.5, std=0.5):\n","    # select a sample or create grid if img is a batch\n","    if len(img.shape) == 4:\n","        nrows = nrows if nrows is not None else int(math.sqrt(img.size(0)))\n","        img = utils.make_grid(img, nrow=nrows)\n","\n","    # unnormalize\n","    img = img.cpu().float()\n","    img = (img*std+mean)*255\n","\n","    # to numpy\n","    image_numpy = img.numpy()\n","    image_numpy = np.transpose(image_numpy, (1, 2, 0))\n","    display(Image.fromarray(image_numpy.astype(imtype))) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P1AaRIWR9L5m","colab_type":"text"},"source":["## Dataset\n","Download and prepare dataset\n"]},{"cell_type":"code","metadata":{"id":"UPMYeOfU9SyD","colab_type":"code","colab":{}},"source":["def mnist():\n","    tf = transforms.Compose(\n","        [\n","            transforms.Resize(32, interpolation=0),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5,), (.5,))\n","        ])\n","    return datasets.MNIST(root='./data/', train=True, transform=tf, download=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QDFqjh4T9a7W","colab_type":"text"},"source":["## Data Loader\n","Create a data loader for the MNIST dataset"]},{"cell_type":"code","metadata":{"id":"TKNGNOBQ9U4w","colab_type":"code","colab":{}},"source":["batch_size = 100\n","data_loader = torch.utils.data.DataLoader(mnist(), batch_size=batch_size, shuffle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MFEBhsZb96T7","colab_type":"text"},"source":["## Networks\n","First, lets define our simple generator"]},{"cell_type":"code","metadata":{"id":"yA2vnc-G9-Nl","colab_type":"code","colab":{}},"source":["class Generator(torch.nn.Module):\n","    \n","    def __init__(self):\n","        super(Generator, self).__init__()\n","      \n","        self._fc = torch.nn.Linear(100, 1024*4*4)\n","\n","        self._conv1 = nn.Sequential(\n","            nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(inplace=True)\n","        )\n","        self._conv2 = nn.Sequential(\n","            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True)\n","        )\n","        self._conv3 = nn.Sequential(\n","            nn.ConvTranspose2d(in_channels=256, out_channels=1, kernel_size=4, stride=2, padding=1, bias=False),\n","            torch.nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        # Noise to image\n","        x = self._fc(x)\n","        x = self._conv1(x.view(x.shape[0], 1024, 4, 4))\n","        x = self._conv2(x)\n","        return self._conv3(x)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LnSOu07Q-FLh","colab_type":"text"},"source":["Similarly lets define a simple discriminator"]},{"cell_type":"code","metadata":{"id":"fI6fdck8-Q2N","colab_type":"code","colab":{}},"source":["class Discriminator(torch.nn.Module):\n","    \n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        \n","        self._conv1 = nn.Sequential(\n","            nn.Conv2d(in_channels=1, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True)\n","        )\n","        self._conv2 = nn.Sequential(\n","            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True)\n","        )\n","        self._conv3 = nn.Sequential(\n","            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.2, inplace=True)\n","        )\n","        \n","        self._fc = nn.Sequential(\n","            nn.Linear(512*4*4, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        # Classify image as fake or real\n","        x = self._conv1(x)\n","        x = self._conv2(x)\n","        x = self._conv3(x)\n","        return self._fc(x.view(-1, 512*4*4))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ZVzk2jo-X9E","colab_type":"text"},"source":["## Model\n","Now lets create the core of our task, the model.  Remember GANs loss:\n","\n","$\\underset{G}{\\text{min}} \\underset{D}{\\text{max}}V(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)}\\big[logD(x)\\big] + \\mathbb{E}_{z\\sim p_{z}(z)}\\big[log(1-D(G(z)))\\big]$"]},{"cell_type":"code","metadata":{"id":"tL2y10N0-Y4U","colab_type":"code","colab":{}},"source":["class Model:\n","    # --------------------------\n","    # -- PREPARE MODEL\n","    # --------------------------\n","    \n","    def __init__(self, batch_size):\n","        self._create_networks()\n","        self._create_optimizer()\n","        self._init_criterion(batch_size)\n","        \n","    def _create_networks(self):\n","        # create networks\n","        self._generator = Generator()\n","        self._discriminator = Discriminator()\n","        \n","        # init weights\n","        self._generator.apply(init_weights)\n","        self._discriminator.apply(init_weights)\n","        \n","        # move to cuda\n","        self._generator.cuda()\n","        self._discriminator.cuda()\n","        \n","    def _create_optimizer(self):\n","        # generator optimizer\n","        self._opt_g = torch.optim.Adam(self._generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","        \n","        # discriminator optimizer\n","        self._opt_d = torch.optim.Adam(self._discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","        \n","    def _init_criterion(self, batch_size):\n","        self._criterion = nn.BCELoss()\n","        self._label_real = Variable(torch.ones(batch_size, 1)).cuda()\n","        self._label_fake = Variable(torch.zeros(batch_size, 1)).cuda()\n","        \n","    # --------------------------\n","    # -- Generate Fake Samples\n","    # --------------------------\n","    \n","    def generate_samples(self, batch_size, z=None):\n","        # sample random noise\n","        if z is None:\n","            z = norm_noise(batch_size)\n","            \n","        # TODO: generate fake samples out of the random noise z\n","        fake_samples = ...\n","        return fake_samples\n","    \n","    # --------------------------\n","    # -- Optimize Model\n","    # --------------------------\n","        \n","    def step_optimization(self, real_samples):\n","        # generate fake samples\n","        fake_samples = self.generate_samples(real_samples.size(0))\n","        \n","        # optimize generator\n","        loss_g = self._step_opt_g(fake_samples)\n","        \n","        # optimize discriminator\n","        loss_d = self._step_opt_d(real_samples, fake_samples.detach())\n","        \n","        return loss_g, loss_d\n","      \n","    def _step_opt_g(self, fake_samples):\n","        # Reset gradients\n","        self._opt_g.zero_grad()\n","        \n","        # TODO: Calculate generator loss\n","        estim_fake = ...\n","        loss = self._criterion(estim_fake, ...)\n","        loss.backward()\n","        \n","        # Update weights\n","        self._opt_g.step()\n","        \n","        return loss.item()\n","        \n","    def _step_opt_d(self, real_samples, fake_samples):\n","        # Reset gradients\n","        self._opt_d.zero_grad()\n","\n","        # TODO: Calculate discriminator loss for real samples\n","        estim_real = ...\n","        loss_real = self._criterion(estim_real, ...)\n","\n","        # TODO: Calculate discriminator loss for fake samples\n","        estim_fake = ...\n","        loss_fake = self._criterion(estim_fake, ...)\n","        \n","        # Total discriminator loss\n","        loss = (loss_real + loss_fake) / 2\n","        loss.backward()\n","\n","        # Update weights\n","        self._opt_d.step()\n","\n","        return loss_real.item(), loss_fake.item()\n","        \n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iWS0uvRJEHGd","colab_type":"text"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"knwG43nIEJM8","colab_type":"code","colab":{}},"source":["num_epochs = 300\n","num_val_samples = 25\n","z_val = norm_noise(num_val_samples)\n","model = Model(batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pd3efLyTEQSE","colab_type":"code","colab":{}},"source":["for epoch in range(num_epochs):\n","  \n","    # Train epoch\n","    for n_batch, (real_samples,_) in enumerate(data_loader):\n","        \n","        # Prepare batch data\n","        real_samples = Variable(real_samples).cuda()\n","          \n","        # Update model weights\n","        loss_g, loss_d = model.step_optimization(real_samples)\n","        \n","        # Show current loss\n","        if (n_batch) % 10 == 0:\n","            print(f\"epoch: {epoch}/{num_epochs}, batch: {n_batch}/{len(data_loader)}, G_loss: {loss_g}, D_loss: {loss_d}\")    \n","          \n","        # Show fake samples\n","        if (n_batch) % 100 == 0:\n","            val_fake_samples = model.generate_samples(num_val_samples, z=z_val).data.cpu()\n","            display_batch_images(val_fake_samples)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TSsyAao5f0Dw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}